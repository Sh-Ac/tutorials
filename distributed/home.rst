Distributed and Parallel Training Tutorials
===========================================

This page includes all distributed and parallel trainings available
at pytorch.org website. 

Getting Started with Distributed Data-Parallel Training (DDP)
-------------------------------------------------------------

.. grid:: 3

     .. grid-item-card:: Getting Started with PyTorch Distributed
        :shadow: none
        :link: https://example.com
        :link-type: url
            
        This tutorial provides a gentle intro to the PyTorch
        DistributedData Parallel.

     .. grid-item-card:: Single Machine Model Parallel Best Practices
        :shadow: none
        :link: https://example.com
        :link-type: url

        In this tutorial you will learn about best practices in
        using model parallel.

     .. grid-item-card:: Writing Distributed Applications with PyTorch
        :shadow: none
        :link: https://example.com
        :link-type: url

        This tutorial demonstrates how to write a distributed application
        with PyTorch.

Learn FSDP
----------

Fully-Sharded Data Parallel (FSDP) is a tool that distributes model
parameters across multiple workers, therefore enabling you to train larger
models.


.. grid:: 3

     .. grid-item-card:: Getting Started with FSDP
        :shadow: none
        :img-top: ../_static/img/thumbnails/cropped/pytorch-logo.png
        :link: https://example.com
        :link-type: url

        This tutorial provides a gentle intro to the PyTorch
        DistributedData Parallel.

     .. grid-item-card:: Single Machine Model Parallel Best Practices
        :shadow: none
        :img-top: ../_static/img/thumbnails/cropped/pytorch-logo.png
        :link: https://example.com
        :link-type: url

        In this tutorial you will learn about best practices in
        using model parallel.

     .. grid-item-card:: Writing Distributed Applications with PyTorch
        :shadow: none
        :img-top: ../_static/img/thumbnails/cropped/pytorch-logo.png
        :link: https://example.com
        :link-type: url

        This tutorial demonstrates how to write a distributed application
        with PyTorch.

Learn RPC
---------

Distributed Remote Procedure Call (RPC) framework provides
mechanisms for multi-machine model training
