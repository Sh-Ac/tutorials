Distributed and Parallel Training Tutorials
===========================================

Distributed and 
at pytorch.org website. 

Getting Started with Distributed Data-Parallel Training (DDP)
-------------------------------------------------------------



.. grid:: 3

     .. grid-item-card:: :octicon:`file-code;1em` 
        Getting Started with PyTorch Distributed
        :shadow: none
        :link: https://example.com
        :link-type: url
            
        This tutorial provides a gentle intro to the PyTorch
        DistributedData Parallel.
 
        :octicon:`code;1em` Code 

     .. grid-item-card:: :octicon:`file-code;1em`
        Single Machine Model Parallel Best Practices
        :shadow: none
        :link: https://example.com
        :link-type: url

        In this tutorial you will learn about best practices in
        using model parallel.

        :octicon:`code;1em` Code :octicon:`square-fill;1em`  :octicon:`video;1em` Video

     .. grid-item-card:: :octicon:`file-code;1em` Writing Distributed Applications with PyTorch
        :shadow: none
        :link: https://example.com
        :link-type: url

        This tutorial demonstrates how to write a distributed application
        with PyTorch.

        :octicon:`code;1em` Code :octicon:`square-fill;1em` :octicon:`video;1em` Video

Learn FSDP
----------

Fully-Sharded Data Parallel (FSDP) is a tool that distributes model
parameters across multiple workers, therefore enabling you to train larger
models.


.. grid:: 3

Learn RPC
---------

Distributed Remote Procedure Call (RPC) framework provides
mechanisms for multi-machine model training
